# ğŸ¤– RAG AvanÃ§ado - Guia Completo

## ğŸ“š O que Ã© RAG?

**RAG = Retrieval-Augmented Generation**

Ã‰ uma tÃ©cnica que combina:
1. **Retrieval** (Busca): Encontrar informaÃ§Ãµes relevantes em sua base de conhecimento
2. **Augmented** (Aumentado): Enriquecer o prompt do LLM com esse contexto
3. **Generation** (GeraÃ§Ã£o): LLM gera resposta baseada no contexto recuperado

### ğŸ’¡ Analogia Simples:

**Sem RAG:**
- VocÃª: "Claude, como funciona o selector21?"
- Claude: "NÃ£o sei, nÃ£o tenho contexto sobre seu cÃ³digo"

**Com RAG:**
- VocÃª: "Claude, como funciona o selector21?"
- Sistema busca: [3 conversas relevantes + cÃ³digo]
- Sistema monta prompt: "Context: [conversas]... User: como funciona selector21?"
- Claude: "Baseado no contexto, o selector21 funciona assim..."

---

## ğŸ¯ Seu Sistema Atual vs RAG Completo

### âœ… O que vocÃª JÃ TEM (RAG Parcial):

| Componente | Status | DescriÃ§Ã£o |
|------------|--------|-----------|
| **Armazenamento** | âœ… Pronto | mem0 + SQLite |
| **Embeddings** | âœ… Pronto | Busca semÃ¢ntica vetorial |
| **Retrieval** | âœ… Pronto | `memory search` |
| **Auto-indexaÃ§Ã£o** | âœ… Pronto | Auto-sync 10min |

### âŒ O que FALTA para RAG AvanÃ§ado:

| Componente | Status | DescriÃ§Ã£o |
|------------|--------|-----------|
| **Query Expansion** | âŒ Falta | Gerar variaÃ§Ãµes da pergunta |
| **Hybrid Search** | âŒ Falta | Vetorial + Keyword (BM25) |
| **Re-ranking** | âŒ Falta | Reordenar por relevÃ¢ncia real |
| **Compression** | âŒ Falta | Remover partes irrelevantes |
| **Prompt Builder** | âŒ Falta | Montar prompt estruturado |
| **LLM Integration** | âŒ Falta | Auto-chamada Claude/GPT |

---

## ğŸš€ Features do RAG AvanÃ§ado

### 1. ğŸ¯ Retrieval Sofisticado

#### a) **Hybrid Search** (Busca HÃ­brida)

Combina 2 mÃ©todos:
- **Vetorial**: Busca por similaridade semÃ¢ntica (embeddings)
- **Keyword**: Busca por palavras exatas (BM25, SQLite FTS)

**Exemplo:**
```python
# Busca vetorial
vector_results = mem0.search("selector21 implementaÃ§Ã£o")  # Top 50

# Busca keyword
keyword_results = sqlite_fts.search("selector21")  # Top 50

# Merge e dedup
final_results = merge_and_deduplicate(vector_results, keyword_results)  # Top 100
```

**Vantagem:** Pega tanto matches semÃ¢nticos quanto exatos

#### b) **Multi-Query**

Gera variaÃ§Ãµes da pergunta para melhor cobertura:

**Query original:** "Como funciona selector21?"

**VariaÃ§Ãµes geradas:**
- "Arquitetura do selector21"
- "ImplementaÃ§Ã£o do selector21"
- "Como selector21 seleciona estratÃ©gias"
- "Bugs e fixes do selector21"

```python
from openai import OpenAI

def expand_query(query: str) -> list[str]:
    prompt = f"""Gere 4 variaÃ§Ãµes desta pergunta para buscar em documentaÃ§Ã£o tÃ©cnica:
    
    Query: {query}
    
    Retorne apenas as 4 queries, uma por linha."""
    
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.choices[0].message.content.split('\n')
```

#### c) **HyDE** (Hypothetical Document Embeddings)

TÃ©cnica avanÃ§ada:
1. LLM gera resposta hipotÃ©tica (mesmo sem saber resposta real)
2. Usa resposta hipotÃ©tica para buscar docs similares
3. Mais efetivo que buscar com query direta

**Exemplo:**
```python
# 1. Gerar documento hipotÃ©tico
hypothetical_doc = llm.generate(
    "Escreva uma explicaÃ§Ã£o tÃ©cnica sobre como o selector21 funciona"
)
# Resultado: "O selector21 Ã© um sistema que usa ML para..."

# 2. Buscar docs similares ao documento hipotÃ©tico
results = vector_search(hypothetical_doc)
# Encontra: documentos reais que falam sobre selector21
```

**Por que funciona?** Documentos tendem a ser similares entre si, entÃ£o buscar por um "documento fake" encontra documentos reais.

---

### 2. ğŸ” Re-Ranking

**Problema:** Busca vetorial inicial Ã© rÃ¡pida mas imprecisa

**SoluÃ§Ã£o:** Re-ranker (cross-encoder)

**Fluxo:**
```
1. Busca inicial (vetorial): 100 docs (rÃ¡pida, ~70% precisÃ£o)
2. Re-rank (cross-encoder): Top 10 (lenta, ~95% precisÃ£o)
```

**ImplementaÃ§Ã£o:**
```python
from sentence_transformers import CrossEncoder

# 1. Busca inicial (rÃ¡pida)
initial_results = vector_search(query, top_k=100)

# 2. Re-rank (precisa)
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
pairs = [(query, doc.content) for doc in initial_results]
scores = reranker.predict(pairs)

# 3. Reordenar por score
sorted_results = sorted(
    zip(initial_results, scores),
    key=lambda x: x[1],
    reverse=True
)[:10]  # Top 10
```

**DiferenÃ§a:**
- **Bi-encoder** (vetorial): Compara embeddings prÃ©-calculados (rÃ¡pido)
- **Cross-encoder** (re-rank): Avalia query + doc juntos (preciso)

---

### 3. ğŸ§© Chunking Inteligente

**Problema:** Como dividir documentos grandes?

**EstratÃ©gias:**

#### a) Fixed-size chunking (Simples)
```python
chunk_size = 500  # tokens
overlap = 50      # tokens de overlap

chunks = split_into_chunks(document, chunk_size, overlap)
```

#### b) Semantic chunking (Inteligente)
```python
# Quebra em frases naturais
# Agrupa frases semanticamente similares
chunks = semantic_split(document)
```

#### c) Document hierarchy (Melhor)
```python
{
    "section": "ImplementaÃ§Ã£o selector21",
    "subsection": "Feature Engineering",
    "content": "...",
    "metadata": {
        "file": "selector.py",
        "function": "engineer_features",
        "lines": "45-120"
    }
}
```

**Seu caso:** Conversas jÃ¡ sÃ£o "chunks" naturais! âœ…

---

### 4. ğŸ”— Contextual Compression

**Problema:** Contexto muito grande â†’ custo alto + perde foco

**SoluÃ§Ã£o:** Comprimir mantendo sÃ³ o relevante

```python
def compress_context(docs: list, query: str) -> str:
    compressed = []
    
    for doc in docs:
        # LLM extrai apenas partes relevantes para query
        relevant_parts = llm.extract_relevant(doc, query)
        compressed.append(relevant_parts)
    
    return "\n\n".join(compressed)

# Exemplo
original = "5000 tokens"  # 3 conversas completas
compressed = "1500 tokens"  # Apenas partes relevantes
# Economia: 70% tokens = $$$
```

---

### 5. ğŸ­ Multi-Hop Reasoning

**Problema:** Perguntas complexas precisam mÃºltiplas buscas

**Exemplo:**
```
Query: "Compare selector21 atual com versÃ£o anterior"

â†’ Busca 1: "selector21 implementaÃ§Ã£o atual"
   Resultado: [Docs sobre versÃ£o atual]

â†’ Busca 2: "selector21 versÃ£o anterior histÃ³rico"
   Resultado: [Docs sobre versÃ£o antiga]

â†’ Busca 3: "diferenÃ§as mudanÃ§as selector21"
   Resultado: [Docs sobre o que mudou]

â†’ LLM sintetiza: ComparaÃ§Ã£o completa
```

**ImplementaÃ§Ã£o:**
```python
def multi_hop_rag(query: str, max_hops: int = 3):
    context = []
    current_query = query
    
    for hop in range(max_hops):
        # 1. Buscar
        results = search(current_query)
        context.extend(results)
        
        # 2. LLM decide: precisa mais informaÃ§Ã£o?
        needs_more = llm.needs_more_context(query, context)
        if not needs_more:
            break
        
        # 3. Gerar prÃ³xima query
        current_query = llm.generate_followup_query(query, context)
    
    # 4. Resposta final
    return llm.generate_answer(query, context)
```

---

### 6. ğŸŒ³ Graph RAG

**Usa grafo de conhecimento para navegaÃ§Ã£o inteligente**

**VocÃª jÃ¡ tem a base!** mem0 armazena:
- Entidades (nodes)
- RelaÃ§Ãµes (edges)

**Como usar:**
```python
# Query: "O que mudou no selector21?"

# 1. Encontrar entidade
entity = graph.find_entity("selector21")

# 2. Traversal no grafo
related = graph.get_related(
    entity,
    relations=["modified_by", "references", "replaced"]
)

# 3. Buscar contexto de cada node
docs = [retrieve_context(node) for node in related]

# 4. LLM sintetiza
answer = llm.generate(query, docs)
```

**Vantagem:** Segue relaÃ§Ãµes estruturadas, nÃ£o apenas similaridade

---

### 7. ğŸ¨ Self-RAG

**LLM decide automaticamente:**
- Quando buscar mais info
- Se resposta estÃ¡ boa
- Se precisa iterar

```python
def self_rag(query: str):
    # 1. LLM tenta responder
    initial_answer = llm.generate(query, context=[])
    
    # 2. LLM avalia prÃ³pria resposta
    confidence = llm.evaluate_confidence(initial_answer)
    
    if confidence < 0.7:
        # 3. Buscar mais contexto
        context = search(query)
        
        # 4. Tentar novamente
        better_answer = llm.generate(query, context)
        
        # 5. Verificar fontes
        is_grounded = llm.check_grounding(better_answer, context)
        
        if is_grounded:
            return better_answer
        else:
            return "NÃ£o encontrei informaÃ§Ã£o suficiente"
    
    return initial_answer
```

---

## ğŸ“Š Arquitetura Completa para Seu Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. USER QUERY                                â”‚
â”‚            "Como funciona o selector21?"                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 2. QUERY PROCESSOR                              â”‚
â”‚  â€¢ Expand query (GPT-4 mini): 4 variaÃ§Ãµes                      â”‚
â”‚  â€¢ Extract entities: ["selector21"]                            â”‚
â”‚  â€¢ Generate embeddings                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 3. HYBRID RETRIEVAL                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Vector Search      â”‚     â”‚  Keyword Search     â”‚          â”‚
â”‚  â”‚  (mem0 embeddings)  â”‚     â”‚  (SQLite FTS5)      â”‚          â”‚
â”‚  â”‚  Top 50 docs        â”‚     â”‚  Top 50 docs        â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚             â”‚                           â”‚                      â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                       â”‚                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚              â”‚  Merge + Dedup   â”‚                             â”‚
â”‚              â”‚  100 unique docs â”‚                             â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    4. RE-RANKER                                 â”‚
â”‚  â€¢ Cross-encoder model (local)                                 â”‚
â”‚  â€¢ Score each doc vs query                                     â”‚
â”‚  â€¢ Sort by relevance score                                     â”‚
â”‚  â€¢ Keep top 10                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              5. CONTEXTUAL COMPRESSION                          â”‚
â”‚  â€¢ LLM extracts relevant parts                                 â”‚
â”‚  â€¢ Removes noise and irrelevant info                           â”‚
â”‚  â€¢ Reduces tokens: 5000 â†’ 1500                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 6. PROMPT BUILDER                               â”‚
â”‚  System: VocÃª Ã© expert em trading bots Python                  â”‚
â”‚                                                                  â”‚
â”‚  Context (from memory):                                         â”‚
â”‚  [Doc1] selector21 implementaÃ§Ã£o inicial...                    â”‚
â”‚  [Doc2] refatoraÃ§Ã£o para ML-based selection...                 â”‚
â”‚  [Doc3] bug fix na validaÃ§Ã£o temporal...                       â”‚
â”‚                                                                  â”‚
â”‚  User: Como funciona o selector21?                             â”‚
â”‚                                                                  â”‚
â”‚  Instructions: Responda APENAS baseado no Context.             â”‚
â”‚  Cite as fontes. Se nÃ£o souber, diga.                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   7. LLM GENERATION                             â”‚
â”‚  â€¢ Claude Sonnet 4.5                                           â”‚
â”‚  â€¢ Generates answer from context                               â”‚
â”‚  â€¢ Cites sources                                               â”‚
â”‚  â€¢ Flags uncertainties                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   8. FINAL RESPONSE                             â”‚
â”‚                                                                  â”‚
â”‚  O selector21 funciona atravÃ©s de 3 componentes principais:    â”‚
â”‚                                                                  â”‚
â”‚  1. **Feature Engineering** [1]: Calcula 47 features...        â”‚
â”‚  2. **ML Scoring** [2]: Random Forest treinado com...         â”‚
â”‚  3. **Validation** [3]: Walk-forward temporal...              â”‚
â”‚                                                                  â”‚
â”‚  Sources:                                                       â”‚
â”‚  [1] Conversa 2025-10-15: ImplementaÃ§Ã£o inicial               â”‚
â”‚  [2] Conversa 2025-11-03: RefatoraÃ§Ã£o ML                      â”‚
â”‚  [3] Conversa 2025-11-10: Bug fix validaÃ§Ã£o                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ ImplementaÃ§Ã£o PrÃ¡tica

### OpÃ§Ã£o 1: Framework Pronto (Mais RÃ¡pido)

#### **LangChain** (Recomendado)
```python
pip install langchain chromadb anthropic

# CÃ³digo mÃ­nimo
from langchain.vectorstores import Chroma
from langchain.llms import Anthropic
from langchain.chains import RetrievalQA

# 1. Setup
vectorstore = Chroma(persist_directory="/home/scalp/memory/chroma")
llm = Anthropic(model="claude-sonnet-4")

# 2. Create RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 10}),
    return_source_documents=True
)

# 3. Query
result = qa_chain("Como funciona o selector21?")
print(result["result"])
print("Sources:", result["source_documents"])
```

#### **LlamaIndex** (Mais focado em RAG)
```python
pip install llama-index

from llama_index import (
    VectorStoreIndex,
    ServiceContext,
    StorageContext,
    load_index_from_storage
)

# 1. Setup
storage_context = StorageContext.from_defaults(
    persist_dir="/home/scalp/memory/llama_index"
)
index = load_index_from_storage(storage_context)

# 2. Create query engine (RAG automÃ¡tico!)
query_engine = index.as_query_engine(
    similarity_top_k=10,
    response_mode="tree_summarize"
)

# 3. Query
response = query_engine.query("Como funciona o selector21?")
print(response)
```

---

### OpÃ§Ã£o 2: Custom (Mais Controle)

```python
# rag_system.py
import anthropic
from sentence_transformers import CrossEncoder
from typing import List, Dict

class AdvancedRAG:
    def __init__(self):
        self.claude = anthropic.Anthropic()
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        
    def query(self, user_query: str) -> Dict:
        # 1. Query Expansion
        expanded_queries = self.expand_query(user_query)
        
        # 2. Hybrid Search
        vector_results = self.vector_search(expanded_queries)
        keyword_results = self.keyword_search(user_query)
        all_results = self.merge_results(vector_results, keyword_results)
        
        # 3. Re-rank
        reranked = self.rerank(user_query, all_results, top_k=10)
        
        # 4. Compress
        compressed = self.compress_context(user_query, reranked)
        
        # 5. Build Prompt
        prompt = self.build_prompt(user_query, compressed)
        
        # 6. Generate
        response = self.generate(prompt)
        
        return {
            "answer": response,
            "sources": [doc["metadata"] for doc in reranked],
            "num_docs_retrieved": len(all_results),
            "num_docs_used": len(reranked)
        }
    
    def expand_query(self, query: str) -> List[str]:
        prompt = f"""Generate 3 variations of this query for better retrieval:

Query: {query}

Return only the 3 queries, one per line."""
        
        message = self.claude.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=200,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return message.content[0].text.split('\n')
    
    def vector_search(self, queries: List[str]) -> List[Dict]:
        # Busca em mem0 para cada query
        # Implementar com seu sistema atual
        pass
    
    def keyword_search(self, query: str) -> List[Dict]:
        # SQLite FTS5
        # SELECT * FROM docs WHERE content MATCH ?
        pass
    
    def rerank(self, query: str, docs: List[Dict], top_k: int) -> List[Dict]:
        pairs = [(query, doc["content"]) for doc in docs]
        scores = self.reranker.predict(pairs)
        
        ranked = sorted(
            zip(docs, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [doc for doc, score in ranked[:top_k]]
    
    def compress_context(self, query: str, docs: List[Dict]) -> str:
        # Opcional: usar LLM para comprimir
        # Por ora, apenas truncar
        max_tokens = 3000
        compressed = []
        
        for doc in docs:
            compressed.append(doc["content"][:500])  # Primeiros 500 chars
        
        return "\n\n---\n\n".join(compressed)
    
    def build_prompt(self, query: str, context: str) -> str:
        return f"""You are an expert assistant for a trading bot developer.

Context from previous conversations and code:
{context}

User Question: {query}

Instructions:
- Answer ONLY based on the Context provided
- Cite sources using [N] notation
- If you don't know, say "I don't have enough information"
- Be technical and precise

Answer:"""
    
    def generate(self, prompt: str) -> str:
        message = self.claude.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return message.content[0].text

# Uso
rag = AdvancedRAG()
result = rag.query("Como funciona o selector21?")
print(result["answer"])
print("\nSources:", result["sources"])
```

---

## ğŸ’° Custo vs BenefÃ­cio

### RAG BÃ¡sico (vocÃª tem):
- **Custo:** $0/mÃªs
- **Setup:** Pronto!
- **PrecisÃ£o:** ~70%
- **Uso:** Manual (vocÃª lÃª e interpreta)

### RAG AvanÃ§ado:
- **Custo:** $5-20/mÃªs (depende do uso)
- **Setup:** 2-4 dias dev
- **PrecisÃ£o:** ~90-95%
- **Uso:** AutomÃ¡tico (LLM responde diretamente)

#### Breakdown de Custos:

| Componente | Modelo | Custo/query |
|------------|--------|-------------|
| Query Expansion | GPT-4o mini | $0.001 |
| Re-ranker | Local (grÃ¡tis) | $0 |
| Compression | Claude Haiku | $0.002 |
| Generation | Claude Sonnet | $0.015 |
| **Total** | | **$0.018/query** |

**Estimativa:**
- 50 queries/dia = $0.90/dia = **$27/mÃªs**
- 20 queries/dia = $0.36/dia = **$11/mÃªs**
- 5 queries/dia = $0.09/dia = **$2.70/mÃªs**

### Vale a pena?

**Para trading bot:** SIM! ğŸ’¯

**Por quÃª?**
- âœ… 1 bug evitado = economiza 2-4 horas
- âœ… 1 estratÃ©gia otimizada = potencial $$$
- âœ… Conhecimento sempre acessÃ­vel
- âœ… Time-to-market mais rÃ¡pido
- âœ… Onboarding de novos devs facilitado

**ROI:** Se economizar 1 hora/semana = ~$100-200 valor/hora = **ROI 10-40x**

---

## ğŸ¯ Quando Usar RAG AvanÃ§ado?

### âœ… Use RAG AvanÃ§ado quando:
- Base de conhecimento grande (>1000 docs)
- Queries complexas (multi-hop)
- PrecisÃ£o crÃ­tica (decisÃµes de $$$)
- Respostas longas e detalhadas
- CitaÃ§Ã£o de fontes obrigatÃ³ria
- MÃºltiplos usuÃ¡rios (time)

### âš ï¸ RAG BÃ¡sico suficiente quando:
- Base pequena (<100 docs)
- Queries simples (lookup)
- VocÃª valida manualmente
- Respostas curtas
- Apenas exploraÃ§Ã£o pessoal

### Para seu caso (Trading Bot):
**â†’ RAG AvanÃ§ado faz sentido!** âœ…

**Justificativa:**
- âœ… Muitas conversas tÃ©cnicas (>100)
- âœ… CÃ³digo complexo (Python + ML)
- âœ… DecisÃµes crÃ­ticas (dinheiro real)
- âœ… HistÃ³rico de bugs/fixes importante
- âœ… EvoluÃ§Ã£o contÃ­nua de estratÃ©gias
- âœ… DocumentaÃ§Ã£o viva necessÃ¡ria

---

## ğŸš€ PrÃ³ximos Passos

### Fase 1: Setup BÃ¡sico (1-2 dias)
1. âœ… Instalar framework (LangChain ou LlamaIndex)
2. âœ… Integrar com mem0 existente
3. âœ… Testar query simples
4. âœ… Validar respostas

### Fase 2: Features AvanÃ§adas (2-3 dias)
1. âœ… Implementar hybrid search
2. âœ… Adicionar re-ranker
3. âœ… Query expansion
4. âœ… Contextual compression

### Fase 3: ProduÃ§Ã£o (1-2 dias)
1. âœ… API REST endpoint
2. âœ… CLI interface
3. âœ… Logging e monitoring
4. âœ… Caching de resultados

### Fase 4: OtimizaÃ§Ã£o (contÃ­nuo)
1. âœ… Tuning de parÃ¢metros
2. âœ… A/B testing de prompts
3. âœ… Feedback loop
4. âœ… MÃ©tricas de qualidade

---

## ğŸ“š Recursos Adicionais

### Papers importantes:
- **RAG**: https://arxiv.org/abs/2005.11401
- **HyDE**: https://arxiv.org/abs/2212.10496
- **Self-RAG**: https://arxiv.org/abs/2310.11511

### Frameworks:
- **LangChain**: https://python.langchain.com/
- **LlamaIndex**: https://www.llamaindex.ai/
- **Haystack**: https://haystack.deepset.ai/

### Modelos Ãºteis:
- **Embeddings**: `text-embedding-3-large` (OpenAI)
- **Re-ranker**: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- **LLM**: Claude Sonnet 4.5, GPT-4

---

## ğŸ‰ Resumo

**RAG = Transformar sua memÃ³ria passiva em assistente ativo!**

### Seu sistema ATUAL:
```
ğŸ“š Biblioteca â†’ VocÃª busca â†’ VocÃª lÃª â†’ VocÃª interpreta
```

### Com RAG AVANÃ‡ADO:
```
ğŸ¤– BibliotecÃ¡rio Expert â†’ Busca + Resume + Responde automaticamente
```

**Ã‰ o prÃ³ximo nÃ­vel natural do seu sistema profissional!** ğŸš€

---

**Quer implementar? Posso ajudar com cÃ³digo especÃ­fico para seu caso!** ğŸ’ª
